import sys
from main.job.pipeline import PySparkJob

def main():
    job = PySparkJob()

    # Load input data to DataFrame
    print("<<Reading>>")
    observed = job.read_csv(sys.argv[1])
    required = job.read_csv(sys.argv[2])

    # Compute average observed temperature
    print("<<Avg Observed Temperature>>")
    avg_observed_temp = job.calc_average_temperature(observed)
    avg_observed_temp.show()

    # Get faulty plants
    print("<<Faulty Plants>>")
    faulty_plants = job.find_faulty_plants(avg_observed_temp, required)
    faulty_plants.show()

    # Store faulty plants to file
    print("<<Save Faulty Plants>>")
    job.save_as(faulty_plants, sys.argv[3])

    job.init_spark_session().stop()

if __name__ == "__main__":
    main()
