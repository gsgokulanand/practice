Hi Sunder,

As discussed, the sample mockup data looks good. Please proceed with the global control script (CTRL\FWS\AILD\PLD\APP\jobs\SS_RIS\etl\temp\mlr40/mlr40_purge.sh) using the following action point steps. This global script should handle the data purge logic dynamically via parameterized country code ([XX]) input.

‚∏ª

üîÅ Execution Steps:
	1.	Read the table: ic_sail_ss_sg.ss_mlrl40_parmfile and check if the MLR40_purge_flag is set to Y or N.
‚Üí This was created as a sample table with SG entry and a 4-year retention period for unit testing.
	2.	If MLR40_purge_flag = N, skip and move to the next country in the file.
‚Üí If Y, read the ret_period column and store it in a variable (_lg for SG if it‚Äôs 4).
	3.	Get the oldest partition month and year from the tables:
STG_FRONT_OFFICE_TXN_PARTY, MANTAS, TRXN_PARTY, XREF_BASE, and store them in 3 variables.
	4.	Read the variable lg_event (e.g., event=08_202106) and check if it falls within the purge period.
‚Üí If yes, proceed with the input file read step.
	5.	Read the MLR40 input file (column: trxn_intl_id) from NAS and pass the value of each record for table search.
	6.	If the key is found in the table, purge records with the logic below:

‚ö†Ô∏è If not found:

Log the transaction IDs into the audit/log file from the input file (trxn_intl_id) that were not found in the base table.

‚∏ª

Please proceed to develop these 5‚Äì10 steps into a script if required. You can either use a pyspark program or handle this in shell script with HQL steps to achieve the purge logic.

Let me know if any clarifications are required.
